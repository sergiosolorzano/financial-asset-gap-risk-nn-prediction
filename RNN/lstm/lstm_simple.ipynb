{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "Device cuda:0\n",
      "cuda version 12.1\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import floor\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from pyts.image import GramianAngularField\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "#set gpu env\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\",device)\n",
    "print(\"cuda version\",torch.version.cuda)\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyLossLayer:\n",
    "    \"\"\"\n",
    "    Computes square loss with first element of hidden layer array.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def loss(self, pred, label):\n",
    "        pred = pred.to(label.device)\n",
    "        loss = (pred[0][0] - label) ** 2\n",
    "        #print(\"ToyLossLayer pred shape\",pred.shape,\"ToyLossLayer pred[0] shape\",pred[0].shape,\"label shape\",label.shape,\"loss shape\",loss.shape,\"VALUE pred\",pred[0][0],\"VALUE LABEL\",label)\n",
    "        print(\"LOSS:\",loss.shape,\"pred val=\",pred[0][0],\"LABEL val=\",label)\n",
    "        return loss\n",
    "\n",
    "    @classmethod\n",
    "    def bottom_diff(self, pred, label):\n",
    "        pred = pred.to(label.device)\n",
    "        diff = torch.zeros_like(pred)\n",
    "        diff[0] = 2 * (pred[0][0] - label)\n",
    "        #print(\"bottom_diff\",diff[0],\"pred\",pred[0][0],\"label\",label)\n",
    "        print(\"pred[0][0]\",pred[0][0],\"label\",label)\n",
    "        return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "def sigmoid_derivative(values):\n",
    "    # Sigmoid derivative: sigmoid(x) * (1 - sigmoid(x))\n",
    "    sigmoid_values = torch.sigmoid(values)\n",
    "    return sigmoid_values * (1 - sigmoid_values)\n",
    "\n",
    "def tanh_derivative(values):\n",
    "    # Tanh derivative: 1 - tanh(x)^2\n",
    "    return 1.0 - torch.tanh(values) ** 2\n",
    "\n",
    "def rand_arr(a, b, *args, device='cpu'):\n",
    "    # Create a random tensor on the specified device (CPU or CUDA)\n",
    "    return (torch.rand(*args, device=device) * (b - a)) + a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Simple LSTM, Backpropagate without Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "class LstmParam:\n",
    "    def __init__(self, mem_cell_ct, x_dim):\n",
    "        self.device = torch.device(device)\n",
    "        self.mem_cell_ct = mem_cell_ct\n",
    "        self.x_dim = x_dim\n",
    "        #concat to accomodate matrix multiplication of input + memcell by weights\n",
    "        concat_len = x_dim + mem_cell_ct\n",
    "        # weight matrices\n",
    "        self.wg = torch.empty((mem_cell_ct, concat_len), device=self.device)\n",
    "        self.wi = torch.empty((mem_cell_ct, concat_len), device=self.device)\n",
    "        self.wf = torch.empty((mem_cell_ct, concat_len), device=self.device)\n",
    "        self.wo = torch.empty((mem_cell_ct, concat_len), device=self.device)\n",
    "        init.uniform_(self.wg, -0.1, 0.1)\n",
    "        init.uniform_(self.wi, -0.1, 0.1)\n",
    "        init.uniform_(self.wf, -0.1, 0.1)\n",
    "        init.uniform_(self.wo, -0.1, 0.1)\n",
    "        # bias terms\n",
    "        self.bg = torch.empty((mem_cell_ct), device=self.device)\n",
    "        self.bi = torch.empty((mem_cell_ct), device=self.device)\n",
    "        self.bf = torch.empty((mem_cell_ct), device=self.device) \n",
    "        self.bo = torch.empty((mem_cell_ct), device=self.device)\n",
    "        init.uniform_(self.bg, -0.1, 0.1)\n",
    "        init.uniform_(self.bi, -0.1, 0.1)\n",
    "        init.uniform_(self.bf, -0.1, 0.1)\n",
    "        init.uniform_(self.bo, -0.1, 0.1)\n",
    "        # diffs (derivative of loss function w.r.t. all parameters)\n",
    "        self.wg_diff = torch.zeros_like(self.wg)\n",
    "        self.wi_diff = torch.zeros_like(self.wi)\n",
    "        self.wf_diff = torch.zeros_like(self.wf)\n",
    "        self.wo_diff = torch.zeros_like(self.wo)\n",
    "        self.bg_diff = torch.zeros_like(self.bg)\n",
    "        self.bi_diff = torch.zeros_like(self.bi)\n",
    "        self.bf_diff = torch.zeros_like(self.bf)\n",
    "        self.bo_diff = torch.zeros_like(self.bo)\n",
    "\n",
    "    def apply_diff(self, lr = 1):\n",
    "        self.wg -= lr * self.wg_diff\n",
    "        self.wi -= lr * self.wi_diff\n",
    "        self.wf -= lr * self.wf_diff\n",
    "        self.wo -= lr * self.wo_diff\n",
    "        self.bg -= lr * self.bg_diff\n",
    "        self.bi -= lr * self.bi_diff\n",
    "        self.bf -= lr * self.bf_diff\n",
    "        self.bo -= lr * self.bo_diff\n",
    "        # reset diffs to zero\n",
    "        self.wg_diff.zero_()\n",
    "        self.wi_diff.zero_()\n",
    "        self.wf_diff.zero_()\n",
    "        self.wo_diff.zero_()\n",
    "        self.bg_diff.zero_()\n",
    "        self.bi_diff.zero_()\n",
    "        self.bf_diff.zero_()\n",
    "        self.bo_diff.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmState:\n",
    "    def __init__(self, mem_cell_ct, x_dim):\n",
    "        self.device = torch.device(device)\n",
    "        self.mem_cell_ct = mem_cell_ct\n",
    "        #print(\"***********device\",self.device)\n",
    "        self.g = torch.zeros(mem_cell_ct, device=self.device) #input node (tanh xt and ht-1)\n",
    "        self.i = torch.zeros(mem_cell_ct, device=self.device) #input gate (sigmoid xt and ht-1, has gate and multiplies input node)\n",
    "        self.f = torch.zeros(mem_cell_ct, device=self.device) #forget gate\n",
    "        self.o = torch.zeros(mem_cell_ct, device=self.device) #output gate\n",
    "        self.s = torch.zeros(mem_cell_ct, device=self.device) #internal state\n",
    "        self.h = torch.zeros(mem_cell_ct, device=self.device) #value in hidden layers\n",
    "        self.bottom_diff_h = torch.zeros_like(self.h)\n",
    "        #print(\"***At init bottom_diff_h\",self.bottom_diff_h.shape)\n",
    "        self.bottom_diff_s = torch.zeros_like(self.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmNode:\n",
    "    def __init__(self, lstm_param, lstm_state):\n",
    "        # store reference to parameters and to activations\n",
    "        self.state = lstm_state\n",
    "        self.param = lstm_param\n",
    "        # non-recurrent input concatenated with recurrent input\n",
    "        self.xc = None\n",
    "\n",
    "    def bottom_data_is(self, x, s_prev = None, h_prev = None):\n",
    "        #if this is the first lstm node in the network\n",
    "        #Convert to cuda only necessary: x, s_prev, and h_prev if they are already on CUDA\n",
    "        x = x.to(self.state.device)\n",
    "        s_prev = s_prev.to(self.state.device) if s_prev is not None else torch.zeros_like(self.state.s)\n",
    "        h_prev = h_prev.to(self.state.device) if h_prev is not None else torch.zeros_like(self.state.h)\n",
    "\n",
    "        # Reshape h_prev to match the shape for concatenation\n",
    "        #h_prev = h_prev.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # save data for use in backprop\n",
    "        self.s_prev = s_prev\n",
    "        self.h_prev = h_prev\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # concatenate x(t) and h(t-1)\n",
    "        #print(\"h_prev shape before squeeze\",h_prev.shape)\n",
    "        if h_prev.dim() != 2:\n",
    "            h_prev = h_prev.unsqueeze(0)\n",
    "        #print(\"x shape\",x.shape,\"h_prev shape after squeeze\",h_prev.shape)\n",
    "        xc = torch.cat((x, h_prev), dim=1)  # Assuming concatenating along the feature dimension\n",
    "        self.xc = xc\n",
    "        #print(\"*********ENTRY xc.shape\",self.xc.shape)\n",
    "\n",
    "        #print(\"self.param.wg shape\",self.param.wg.shape,\"xc shape\",xc.shape,\"xc shape after unsqueeze\",xc.unsqueeze(1).shape,\"self.param.bg shape\",self.param.bg.shape)\n",
    "        #print(\"self.param.wg shape\",self.param.wg.shape,\"self.param.wg.T shape\",self.param.wg.T.shape,\"xc shape\",xc.shape,\"self.param.bg shape\",self.param.bg.shape)\n",
    "        #self.state.g = np.tanh(np.dot(self.param.wg, xc) + self.param.bg)\n",
    "        self.state.g = torch.tanh(torch.mm(xc, self.param.wg.T).squeeze(1) + self.param.bg)\n",
    "        self.state.i = torch.sigmoid(torch.mm(xc, self.param.wi.T).squeeze(1) + self.param.bi)\n",
    "        self.state.f = torch.sigmoid(torch.mm(xc, self.param.wf.T).squeeze(1) + self.param.bf)\n",
    "        self.state.o = torch.sigmoid(torch.mm(xc, self.param.wo.T).squeeze(1) + self.param.bo)\n",
    "        \n",
    "        # cell state (state.s) carries LT dependencies and modified mininally each step due to gating\n",
    "        self.state.s = self.state.g * self.state.i + s_prev * self.state.f\n",
    "        # hidden state is the node state , carries ST dependencies\n",
    "        self.state.h = self.state.s * self.state.o\n",
    "    \n",
    "    def top_diff_is(self, top_diff_h, top_diff_s):\n",
    "        # No need to convert top_diff_h and top_diff_s if they are already on CUDA\n",
    "        top_diff_h = top_diff_h.to(self.state.device)\n",
    "        top_diff_s = top_diff_s.to(self.state.device)\n",
    "\n",
    "        # notice that top_diff_s is carried along the constant error carousel\n",
    "        ds = self.state.o * top_diff_h + top_diff_s\n",
    "        do = self.state.s * top_diff_h\n",
    "        di = self.state.g * ds\n",
    "        dg = self.state.i * ds\n",
    "        df = self.s_prev * ds\n",
    "\n",
    "        # diffs w.r.t. vector inside sigma / tanh function\n",
    "        #print();print(\"***shape state i\",self.state.i.shape,\"shape di\",di.shape)\n",
    "        di_input = sigmoid_derivative(self.state.i) * di \n",
    "        df_input = sigmoid_derivative(self.state.f) * df \n",
    "        do_input = sigmoid_derivative(self.state.o) * do \n",
    "        dg_input = tanh_derivative(self.state.g) * dg\n",
    "\n",
    "        # diffs w.r.t. inputs\n",
    "        # Ensure di_input, df_input, do_input, dg_input are 2-D before using torch.mm\n",
    "        di_input = di_input.view(1, -1)  # Ensure it's a 2-D tensor\n",
    "        df_input = df_input.view(1, -1)\n",
    "        do_input = do_input.view(1, -1)\n",
    "        dg_input = dg_input.view(1, -1)\n",
    " \n",
    "        #print();print(\"**************di_input shape:\", di_input.T.shape,\"*************self.xc shape:\", self.xc.shape)\n",
    "        \n",
    "        self.param.wi_diff += torch.mm(di_input.T, self.xc)\n",
    "        self.param.wf_diff += torch.mm(df_input.T, self.xc)\n",
    "        self.param.wo_diff += torch.mm(do_input.T, self.xc)\n",
    "        self.param.wg_diff += torch.mm(dg_input.T, self.xc)\n",
    "\n",
    "        self.param.bi_diff += di_input.squeeze(0)\n",
    "        self.param.bf_diff += df_input.squeeze(0)\n",
    "        self.param.bo_diff += do_input.squeeze(0)\n",
    "        self.param.bg_diff += dg_input.squeeze(0)\n",
    "\n",
    "        # Compute bottom diff\n",
    "        dxc = torch.zeros_like(self.xc)\n",
    "        #print(\"#############**********BEFORE shape dxc\",dxc.shape,\"self.xc shape\",self.xc.shape)\n",
    "        dxc += torch.mm(self.param.wi.T, di_input.T).squeeze(1)\n",
    "        dxc += torch.mm(self.param.wf.T, df_input.T).squeeze(1)\n",
    "        dxc += torch.mm(self.param.wo.T, do_input.T).squeeze(1)\n",
    "        dxc += torch.mm(self.param.wg.T, dg_input.T).squeeze(1)\n",
    "        #print();print(\"#############**********AFTER dxc shape\",dxc.shape)\n",
    "\n",
    "        # Save bottom diffs\n",
    "        #print();print(\"######self.state.bottom_diff_s shape\",self.state.bottom_diff_s.shape)\n",
    "        #print();print(\"######BEFORE self.state.bottom_diff_h shape\",self.state.bottom_diff_h.shape)\n",
    "        self.state.bottom_diff_s = ds * self.state.f\n",
    "        #print(\"=============xdim size\",self.param.x_dim)\n",
    "        self.state.bottom_diff_h = dxc[:,self.param.x_dim:]\n",
    "        #print();print(\"######AFTER self.state.bottom_diff_s shape\",self.state.bottom_diff_s.shape)\n",
    "        #print();print(\"######AFTER self.state.bottom_diff_h shape\",self.state.bottom_diff_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmNetwork():\n",
    "    def __init__(self, lstm_param):\n",
    "        self.lstm_param = lstm_param\n",
    "        self.lstm_node_list = []\n",
    "        # input sequence\n",
    "        self.x_list = []\n",
    "\n",
    "    def y_list_is(self, y_list, loss_layer):\n",
    "        \"\"\"\n",
    "        Updates diffs by setting target sequence with corresponding loss layer. \n",
    "        It doesn't update params, need to call self.lstm_param.apply_diff()\n",
    "        \"\"\"\n",
    "        assert len(y_list) == len(self.x_list)\n",
    "        idx = len(self.x_list) - 1\n",
    "\n",
    "        #Convert y_list elements to CUDA tensors\n",
    "        y_list = [torch.tensor(y, device=self.lstm_param.device) for y in y_list]\n",
    "\n",
    "        state_h_numpy_flat_list = self.lstm_node_list[idx].state.h.cpu().detach().numpy().flatten().tolist()\n",
    "        \n",
    "        print(\"***To Calc Loss at idx\",idx,\"state.h count\",len(state_h_numpy_flat_list),\"state.h at node IDX\",self.lstm_node_list[idx].state.h,\"===y_list[idx]\",y_list[idx], \"y_list\",y_list)\n",
    "\n",
    "        # first node only gets diffs from label ...\n",
    "        loss = loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx])\n",
    "        print(\"BEFORE idx>=0 Node LOSS: idx\",idx,\"self.lstm_node_list[idx].state.h\",self.lstm_node_list[idx].state.h,\"y_list[idx]\",y_list[idx])\n",
    "        diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])\n",
    "        # here s is not affecting loss due to h(t+1), hence we set equal to zero\n",
    "        diff_s = torch.zeros(self.lstm_param.mem_cell_ct, device=self.lstm_param.device)\n",
    "        self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)\n",
    "        idx -= 1\n",
    "\n",
    "        ### ... following nodes also get diffs from next nodes, hence we add diffs to diff_h\n",
    "        ### we also propagate error along constant error carousel using diff_s\n",
    "        while idx >= 0:\n",
    "            print(\"AFTER idx>=0 Node LOSS: idx\",idx,\"self.lstm_node_list[idx].state.h\",self.lstm_node_list[idx].state.h,\"y_list[idx]\",y_list[idx])\n",
    "            loss += loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx])\n",
    "            diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])\n",
    "            #print(\"idx\",idx,\"shape diff_h\",diff_h.shape,\"len self.lstm_node_list\",len(self.lstm_node_list),\"shape self.lstm_node_list[idx].state.h\",self.lstm_node_list[idx].state.h.shape,\"shape bottom_diff_h \",self.lstm_node_list[idx + 1].state.bottom_diff_h.shape)\n",
    "            diff_h += self.lstm_node_list[idx + 1].state.bottom_diff_h\n",
    "            diff_s = self.lstm_node_list[idx + 1].state.bottom_diff_s\n",
    "            self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)\n",
    "            idx -= 1\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def x_list_clear(self):\n",
    "        self.x_list = []\n",
    "\n",
    "    def x_list_add(self, x):\n",
    "        #convert to cuda if necessary\n",
    "        x = x if x.device == self.lstm_param.device else x.to(self.lstm_param.device)\n",
    "        \n",
    "        self.x_list.append(x)\n",
    "        if len(self.x_list) > len(self.lstm_node_list):\n",
    "            # need to add new lstm node, create new state mem\n",
    "            lstm_state = LstmState(self.lstm_param.mem_cell_ct, self.lstm_param.x_dim)\n",
    "            self.lstm_node_list.append(LstmNode(self.lstm_param, lstm_state))\n",
    "\n",
    "        # get index of most recent x input\n",
    "        idx = len(self.x_list) - 1\n",
    "        if idx == 0:\n",
    "            # no recurrent inputs yet\n",
    "            self.lstm_node_list[idx].bottom_data_is(x)\n",
    "        else:\n",
    "            s_prev = self.lstm_node_list[idx - 1].state.s\n",
    "            h_prev = self.lstm_node_list[idx - 1].state.h\n",
    "            self.lstm_node_list[idx].bottom_data_is(x, s_prev, h_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate Through Training Loop:\n",
    "\n",
    "# For each iteration (total of 100 iterations), the function:\n",
    "    # Adds each input vector to the LSTM network, each a node.\n",
    "    # Each node has total num mem cells defined\n",
    "    # Prints the first value of the hidden state h for each node.\n",
    "    # Computes the loss for each node using the y_list_is function.\n",
    "    # Applies the parameter updates.\n",
    "    # Clears the input list of the LSTM network.\n",
    "\n",
    "def run_lstm_simple(train_loader):\n",
    "    # learns to repeat simple sequence from random inputs\n",
    "    np.random.seed(0)\n",
    "\n",
    "    epochs = 100\n",
    "\n",
    "    # parameters for input data dimension and lstm cell count\n",
    "    mem_cell_ct = 100\n",
    "    x_dim = 32*32\n",
    "    lstm_param = LstmParam(mem_cell_ct, x_dim)\n",
    "    lstm_net = LstmNetwork(lstm_param)\n",
    "    #y_list = [-0.5, 0.2, 0.1, -0.5]\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        print(\"epoch\", \"%2s\" % str(epoch), end=\": \")\n",
    "\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            for ind in range(len(inputs)):\n",
    "                print(\"epoch\",epoch,\"inputs[ind] size\",inputs[ind].shape,\"inputs[ind]\",inputs[ind])\n",
    "                print(\"epoch\",epoch,\"labels[ind] size\",labels[ind].shape,\"labels[ind]\",labels[ind])\n",
    "                lstm_net.x_list_add(inputs[ind])\n",
    "\n",
    "            #get all state.h from all nodes in lstm_node_list\n",
    "            y_preds = [node.state.h[0].cpu().detach().numpy() for node in lstm_net.lstm_node_list]\n",
    "            \n",
    "            #flatten to single list\n",
    "            y_preds_flat = [item for sublist in y_preds for item in sublist]\n",
    "            print(\"epoch\",epoch,\"len y_preds_flat i.e. all nodes state.h\",len(y_preds_flat),\"pred = [\" + \", \".join([\"%2.5f\" % pred for pred in y_preds_flat]) + \"]\", end=\", \");print()\n",
    "            print(\"epoch\",epoch,\"y_preds_flat[0] i.e. i.e. 1stNode.state.h =\", y_preds_flat[0])\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = lstm_net.y_list_is(labels, ToyLossLayer)\n",
    "            print(\"loss:\", \"%.3e\" % loss.item())  # Use .item() to get the scalar value from tensor\n",
    "        \n",
    "            lstm_param.apply_diff(lr=0.1)\n",
    "            lstm_net.x_list_clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and encode data to GADF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows 524\n",
      "Num rows for df Close col 524\n",
      "                  Open        High         Low       Close   Adj Close  Volume\n",
      "Date                                                                          \n",
      "2021-10-01  650.000000  669.570007  648.065002  664.530029  664.530029  281112\n",
      "2021-10-04  663.989990  669.000000  645.729797  649.349976  649.349976  340741\n",
      "2021-10-05  657.909973  669.174988  652.072571  665.380005  665.380005  358684\n",
      "2021-10-06  654.039978  663.919983  643.219971  659.849976  659.849976  441251\n",
      "2021-10-07  670.929993  679.000000  663.280029  665.559998  665.559998  258661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#close price time period\n",
    "start_date = '2021-10-01'\n",
    "#start_date = '2023-01-01'\n",
    "end_date = '2023-12-01'\n",
    "\n",
    "ticker = 'SIVBQ'\n",
    "dataset = yf.download(ticker, start=start_date, end=end_date, interval='1d')\n",
    "\n",
    "print(\"num rows\",dataset.shape[0])\n",
    "\n",
    "dataset.dropna(how='any', inplace=True)\n",
    "#print(\"Num rows for df Close col\",len(dataset['Close'].dropna()))\n",
    "print(\"Num rows for df Close col\",len(dataset['Close']))\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging Algorithm: Generate Gramian angular field (GAF) images from time series data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaf_images(dataset, gaf_img_sz=32, method=\"summation\", sample_range=(0,1)):\n",
    "    #print(\"len data series received:\",len(dataset),\"size\",dataset.size)\n",
    "\n",
    "    #determine num of gaf_img_szX images with gaf_img_sz datapoints\n",
    "    num_images_to_generate = floor(len(dataset) / gaf_img_sz)\n",
    "    #print(\"len dataset\",len(dataset),\"num_images_to_generate\",num_images_to_generate)\n",
    "    \n",
    "    #reshape dataset into number of images\n",
    "    dataset = dataset[:num_images_to_generate*gaf_img_sz].reshape(num_images_to_generate, gaf_img_sz)\n",
    "    #print(\"data in GAF\",dataset)\n",
    "    \n",
    "    #TODO: Explore label set according to each feature\n",
    "    #print(\"num_images_to_generate\",\"dataset 1\",dataset[1])\n",
    "    mean_price_list=[]\n",
    "    for i in range(num_images_to_generate):\n",
    "        mean_price_list.append(np.mean(dataset[i]))\n",
    "        #print(\"Num images to generate\",num_images_to_generate,\"Dataset mean\",np.mean(dataset[i]),\"dataset\",dataset)\n",
    "    #print(\"prices in GAF\",price_list)\n",
    "    #print(\"image_size\",gaf_img_sz)\n",
    "    \n",
    "    gaf = GramianAngularField(image_size=gaf_img_sz, method=method, sample_range=sample_range)\n",
    "    gaf_images= gaf.fit_transform(dataset)\n",
    "    #print(\"gaf_image\",gaf_images.shape)\n",
    "    #print(\"returning price list\",mean_price_list)\n",
    "    \n",
    "    return gaf_images, mean_price_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate images for open, high, low, close and adj close prices\n",
    "\n",
    "We transpose the resulting image list to represent:\n",
    "+ 16: number of images\n",
    "+ 5: number of image channels/features. Each image has 5 =\"Open\", \"High\", \"Low\", \"Close\", and \"Adj Close\"\n",
    "+ 32: image height\n",
    "+ 32: image width\n",
    "+ the label is the 32+1 price for each time series chunk (image) corresponding to the relevant feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_feature_num_samples 524\n",
      "Column Open temp image list len to append 480\n",
      "full_feature_num_samples 524\n",
      "Column High temp image list len to append 480\n",
      "full_feature_num_samples 524\n",
      "Column Low temp image list len to append 480\n",
      "full_feature_num_samples 524\n",
      "Column Close temp image list len to append 480\n",
      "full_feature_num_samples 524\n",
      "Column Adj Close temp image list len to append 480\n",
      "shape [0] set (1, 5, 480, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "cols_used = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"]\n",
    "cols_used_count = sum(column_name in cols_used for column_name in dataset.columns)\n",
    "\n",
    "#TODO: Instead of insert a list in each list for each feature, insert the elements directly\n",
    "def generate_multiple_feature_images(dataset, image_size=32, method=\"summation\", sample_range = (0, 1)):\n",
    "    \n",
    "    feature_image_dataset_list=[[] for _ in range(len(cols_used))]\n",
    "    feature_price_dataset_list=[[] for _ in range(len(cols_used))] #=\"Open\", \"High\", \"Low\", \"Close\" , \"Adj Close\"\n",
    "    feature_label_dataset_list=[] #next value for each chunk of =\"Open\", \"High\", \"Low\", \"Close\" , \"Adj Close\"\n",
    "    column_idx = 0\n",
    "\n",
    "    for idx, column_name in enumerate(dataset.columns):\n",
    "\n",
    "      #create open, high, low and close images\n",
    "      if column_name in [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"]:\n",
    "        temp_image_list = []\n",
    "        temp_price_list = []\n",
    "        temp_label_list = []\n",
    "        #print(\"dataset idx\", idx, \"len rows this data feature\", len(dataset[i]), \"dataset[i].shape\", dataset[i].shape, \"dataset i:\", dataset[i])\n",
    "\n",
    "        full_feature_data = dataset[column_name].values\n",
    "        full_feature_num_samples = len(full_feature_data)\n",
    "        print(\"full_feature_num_samples\",full_feature_num_samples)\n",
    "\n",
    "        num_windows = image_size\n",
    "        #add 1 for last window label\n",
    "        window_size = full_feature_num_samples - (image_size + 1)\n",
    "\n",
    "        #loop by data_chunk so each chunk represents the price series that we slide by one day forward\n",
    "        # TODO: parallelism\n",
    "        for curr_window_index in range(num_windows):\n",
    "          \n",
    "          curr_sliding_window_data = full_feature_data[curr_window_index:window_size+curr_window_index]\n",
    "\n",
    "          #print(f\"Curr window len {len(curr_sliding_window_data)} first value {curr_sliding_window_data[0]} next value {curr_sliding_window_data[1]} last value {curr_sliding_window_data[490]}\")\n",
    "          \n",
    "          target_num_chunks = floor(window_size / image_size)\n",
    "          #print(f\"Target number of chunks for curr Window {column_name}\",target_num_chunks)\n",
    "          \n",
    "          for cur_chunk in range(target_num_chunks):\n",
    "            #print(\"cur_chunk\",cur_chunk)\n",
    "            \n",
    "            #chunk size of image size\n",
    "            data_chunk = curr_sliding_window_data[cur_chunk*image_size:(cur_chunk*image_size)+image_size]\n",
    "            #if (cur_chunk == 31 or cur_chunk == 32):\n",
    "              #print(\"cur_chunk\",cur_chunk,\"data chunk\",data_chunk)\n",
    "            #append gaf image to image list. store price feature values in price list\n",
    "            gaf_images, price_list = generate_gaf_images(data_chunk, gaf_img_sz=image_size, method=method, sample_range=sample_range)\n",
    "            temp_image_list.append(gaf_images)\n",
    "            #***print(\"At chunk\",cur_chunk,\"input chunk size\",len(data_chunk),\"shape gaf images\",gaf_images.shape, \"len temp image list\",len(temp_image_list))\n",
    "            \n",
    "            # if(cur_chunk==0):\n",
    "            #   print(\"Price Data Pre-Gaf: i\", cur_chunk, \"len\",len(data_chunk), \"shape\", feature_data.shape, \"data\",data_chunk)\n",
    "            #   print(\"Image Returned: idx\", idx, \"image size\", gaf_images.size, f\"first {image_size} image vals\", gaf_images.flatten()[:image_size])\n",
    "            \n",
    "            temp_price_list.append(price_list)\n",
    "            #print(\"At chunk\",cur_chunk,\"input chunk size\",len(data_chunk),\"len price_list\",len(price_list),price_list)\n",
    "            \n",
    "            #get next single value after the chunk as label to list\n",
    "            #print(\"appending to temp label list\",feature_data[cur_chunk + image_size + 1])\n",
    "            temp_label_list.append(full_feature_data[cur_chunk + image_size + 1])\n",
    "            #feature_label_index_dataset_list.append(feature_data[cur_chunk + image_size + 1])\n",
    "            #print(\"chunk\",cur_chunk,\"label for\",column_name,\"price\",feature_data[cur_chunk + image_size + 1])\n",
    "            #if(column_name == \"Open\"):\n",
    "              #index position for the label of this chunk\n",
    "              #feature_label_index_dataset_list.append(cur_chunk + image_size + 1)\n",
    "              #print(\"at chunk\",cur_chunk,\"feature label list\",feature_label_index_dataset_list)\n",
    "        \n",
    "        print(f\"Column {column_name} temp image list len to append\",len(temp_image_list))\n",
    "        feature_image_dataset_list[column_idx].append(temp_image_list)\n",
    "        #print(\"feature_image_dataset_list\",feature_image_dataset_list)\n",
    "        feature_price_dataset_list[column_idx].append(temp_price_list)\n",
    "        #print(\"price list\",price_list)\n",
    "        feature_label_dataset_list.append(temp_label_list)\n",
    "        column_idx += 1\n",
    "\n",
    "    # print(\"Final len images\",len(feature_image_dataset_list),\n",
    "    #        \"len image list index (i.e. feature) 0\",len(feature_image_dataset_list[0][0]))\n",
    "    # print(\"Final len price list\",len(feature_price_dataset_list),\n",
    "    #        \"len feature_price_dataset_list index 0 (i.e. column 0)\", len(feature_price_dataset_list[0][0]),feature_price_dataset_list)\n",
    "    # print(\"Final len labels\", len(feature_label_dataset_list),feature_label_dataset_list) # 2455=total range*5\n",
    "    \n",
    "    feature_image_dataset_list = np.array(feature_image_dataset_list) \n",
    "    #print(\"Final Shape of images before transpose:\", feature_image_dataset_list.shape)\n",
    "    \n",
    "    #transpose image for CNN\n",
    "    #(5, 1, 491, 1, 32, 32)\n",
    "    feature_image_dataset_list= np.transpose(feature_image_dataset_list, (1, 3, 0, 2, 4, 5))\n",
    "    #print(\"Final Shape of images after transpose:\", feature_image_dataset_list.shape)\n",
    "\n",
    "    return feature_image_dataset_list, feature_price_dataset_list, feature_label_dataset_list\n",
    "\n",
    "#Generate images from dataset\n",
    "gaf_method=\"summation\"\n",
    "sample_range = (0, 1)\n",
    "image_size = 32 #(x,y)\n",
    "feature_image_dataset_list, feature_price_dataset_list, feature_label_dataset_list = generate_multiple_feature_images(dataset, image_size=image_size, method=gaf_method, sample_range=sample_range)\n",
    "\n",
    "print(\"shape [0] set\",np.array(feature_image_dataset_list[0]).shape)\n",
    "\n",
    "np.set_printoptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize image arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_array=np.array(feature_image_dataset_list)\n",
    "prices_array=np.array(feature_price_dataset_list) #not used\n",
    "labels_array=np.array(feature_label_dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =5\n",
    "num_epochs_input = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len img 5 image shape (5, 480, 32, 32)\n",
      "len label 5 labels shape (5, 480)\n"
     ]
    }
   ],
   "source": [
    "#squeeze arrays\n",
    "images_array = images_array.squeeze(axis=(0, 1))\n",
    "print(\"len img\",len(images_array),\"image shape\",images_array.shape)#,\"prices_array[0][0]\",prices_array[0][0])\n",
    "print(\"len label\",len(labels_array),\"labels shape\",labels_array.shape)#,\"prices_array[0][0]\",prices_array[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training/Testing Datasets for Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Transform to image to convert to tensor and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetTransform(normalize_ftor=0.5,resolution_x=32,resolution_y=32):\n",
    "    return transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([normalize_ftor], [normalize_ftor])\n",
    "    #transforms.Resize((resolution_x, resolution_y))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = SetTransform(normalize_ftor=0.5, resolution_x=32, resolution_y=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4D image array shape (5, 480, 32, 32)\n",
      "3D reshaped image array  (2400, 32, 32)\n",
      "labels shape (2400, 1)\n"
     ]
    }
   ],
   "source": [
    "feature_image_dataset_list_f32 = np.array(images_array).astype(np.float32)\n",
    "feature_image_dataset_list_f32 = feature_image_dataset_list_f32.reshape(-1, image_size, image_size)\n",
    "#images_array = np.transpose(feature_image_dataset_list, (1, 0, 2, 3))\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "labels_array = np.array(labels_array)#.astype(np.float32)\n",
    "#print(\"labels array\",labels_array)\n",
    "reshaped_labels_array = labels_array.reshape(-1, 1)\n",
    "#print(\"reshaped labels array\",reshaped_labels_array)\n",
    "labels_scaled_list_f32 = scaler.fit_transform(reshaped_labels_array).reshape(-1,).astype(np.float32)\n",
    "#print(\"scaled labels\",labels_scaled_list_f32)\n",
    "print(\"4D image array shape\",images_array.shape)\n",
    "print(\"3D reshaped image array \",feature_image_dataset_list_f32.shape)\n",
    "print(\"labels shape\",reshaped_labels_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features data (close, high, low, etc) for Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs #all features in one large array\n",
    "        self.labels = labels\n",
    "        self.transform = SetTransform()\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.inputs[index]\n",
    "        Y = self.labels[index]\n",
    "        return X, Y\n",
    "  \n",
    "    def prepare_ordered_dataset(self):\n",
    "        x = []\n",
    "        y = []\n",
    "        #print(\"len inputs\", len(self.inputs), \"shape\", self.inputs.shape, self.inputs.shape[0])\n",
    "        #print(\"len images 0\",len(self.inputs), \"len images 0:\",len(self.inputs[0]))\n",
    "        #print(\"images 0:\",self.inputs[0])\n",
    "        #print(\"labels\",self.labels)\n",
    "        #print(\"len labels\", len(self.labels), self.labels.shape, self.labels.shape[0])\n",
    "        \n",
    "        np.set_printoptions()\n",
    "        for image_num in range(self.inputs.shape[0]):\n",
    "            #print(\"len image data 0\",len(self.inputs[data_window][0]),\"shape\",self.inputs[data_window].shape)\n",
    "            #print(\"label data\",self.labels[image_num][0])\n",
    "            #print(\"imag num:\",image_num)\n",
    "            #print(\"image data at index image_num len:\",len(self.inputs[image_num]))\n",
    "            \n",
    "            self.inputs[image_num] = self.transform(self.inputs[image_num])\n",
    "            \n",
    "            x.append(np.expand_dims(self.inputs[image_num], axis=0))\n",
    "            y.append(self.labels[image_num])\n",
    "            #print(\"img num\",image_num,\"label\",self.labels[image_num])\n",
    "            #print(\"img num\",image_num,\"img\",self.inputs[image_num])\n",
    "            \n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "        #cnn requests labels size (4,1) instead of (4)\n",
    "        y = np.expand_dims(y, axis=1) \n",
    "        #print(\"size self\",self.inputs.shape,self.labels.shape)\n",
    "        #print(\"size self\",len(x),len(y))\n",
    "        dataset = [(img, label) for img, label in zip(x, y)]\n",
    "        #print(\"type dataset returned\",type(dataset), len(dataset), len(dataset[0]), len(dataset[1]))\n",
    "        #print(\"len dataset[0][0]\",len(dataset[0][0][0][0]))\n",
    "        #print(\"len dataset[1][1]\",len(dataset[1][1]))\n",
    "        #print(\"dataset[0]\",dataset[1])\n",
    "        return dataset\n",
    "        \n",
    "        #return np.array(x),np.array(y)\n",
    "    \n",
    "    def split_data(self,dataset, batch_size=batch_size, test_size=0.2, train_shuffle=False):\n",
    "        num_samples = len(dataset)\n",
    "        #print(\"numsamples\",num_samples)\n",
    "        num_test_samples = int(test_size * num_samples)\n",
    "        num_train_samples = num_samples - num_test_samples\n",
    "        #print(\"num_train_samples\",num_train_samples)\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        train_indices = indices[:num_train_samples]\n",
    "        test_indices = indices[num_train_samples:]\n",
    "        #print(\"len train\",len(train_indices),\"len test\",len(test_indices))\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler,shuffle=train_shuffle)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "        #for e in train_loader:\n",
    "            #print(\"train loader ele\",e)\n",
    "\n",
    "        # sample_batch = next(iter(train_loader))\n",
    "        # input_shape = sample_batch[0].shape\n",
    "        # label_shape = sample_batch[1].shape\n",
    "        # print(\"input len\",len(input_shape),\"input shape\",input_shape,\"label len\",len(label_shape))\n",
    "\n",
    "        return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Train_And_Test_Loaders(feature_image_dataset_list_f32,labels_scaled_list_f32, train_shuffle=False, batch_size=batch_size):\n",
    "\n",
    "    #print(\"feature_image_dataset_list_f32[0][0].shape\",feature_image_dataset_list_f32[0][0].shape, \"feature_image_dataset_list_f32[0][0].shape[0]\", feature_image_dataset_list_f32[0][0].shape[0])\n",
    "\n",
    "    #reshape for cnn\n",
    "    #reshaped_feature_image_dataset_list_f32 = np.expand_dims(feature_image_dataset_list_f32[0][0].reshape(-1, *feature_image_dataset_list_f32[0][0].shape[2:]), axis=1)\n",
    "    #print(\"feature_image_dataset_list_f32 shape\",feature_image_dataset_list_f32.shape)\n",
    "    #print(\"res\",reshaped_feature_image_dataset_list_f32.shape)\n",
    "\n",
    "    #generate a list for images and labels\n",
    "    data_prep_class = DataPrep(feature_image_dataset_list_f32, labels_scaled_list_f32)\n",
    "\n",
    "    #print(\"feature_image_dataset_list_f32\",feature_image_dataset_list_f32[0][0].shape)\n",
    "    #print(\"labels_scaled_list_f32\",labels_scaled_list_f32.shape)\n",
    "    #returns list size all observations of all features of size 2:\n",
    "    #(image32x32,label) i.e. shape (5*480,32,32) and (5*480,1)\n",
    "    dataset = data_prep_class.prepare_ordered_dataset()\n",
    "\n",
    "    # for c in range(len(dataset[0])):\n",
    "    #     print(f\"size labels {c}\",dataset[1][c].size)\n",
    "    #     print(f\"size image {c}\",dataset[0][c].shape)\n",
    "\n",
    "    batch_size = batch_size\n",
    "    train_loader, test_loader = data_prep_class.split_data(dataset, \n",
    "                                                         batch_size=batch_size,\n",
    "                                                         train_shuffle=train_shuffle)\n",
    "\n",
    "    # for c,e in enumerate(train_loader):\n",
    "    #     print(\"count\",c)\n",
    "        # print(\"type\",type(e))\n",
    "        # print(\"imga\",e[0].shape)\n",
    "        # print(\"label\",e[1].shape)\n",
    "    #returns 191 train_loaders that contain batch of 10 images32x32 and 10 labels\n",
    "    #=191*10=1910 i.e. 80% of 2400 total\n",
    "    return train_loader,test_loader\n",
    "    \n",
    "\n",
    "train_loader,test_loader = Generate_Train_And_Test_Loaders(feature_image_dataset_list_f32,labels_scaled_list_f32, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lstm_simple(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
