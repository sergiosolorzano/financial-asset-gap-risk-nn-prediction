{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "2.3.0+cu121\n",
      "2.3.0+cu121\n",
      "2.3.0+cu121\n",
      "2.3.0+cu121\n",
      "Device cuda:0\n",
      "cuda version 12.1\n",
      "Device cuda:0\n",
      "cuda version 12.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.backends.cudnn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from bayes_opt import BayesianOptimization\n",
    "from itertools import product\n",
    "\n",
    "#import scripts\n",
    "import importlib as importlib\n",
    "sys.path.append(os.path.abspath('./helper_functions'))\n",
    "import helper_functions.compute_stats as compute_stats\n",
    "import parameters as params\n",
    "import helper_functions.helper_functions as helper_functions\n",
    "import helper_functions.neural_network as neural_network\n",
    "import helper_functions.plot_data as plot_data\n",
    "import helper_functions.load_data as load_data\n",
    "import helper_functions.image_transform as image_transform\n",
    "import helper_functions.generate_images as generate_images\n",
    "\n",
    "#set gpu env\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\",device)\n",
    "print(\"cuda version\",torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params.Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_to_images_process(stock_ticker, params, test_size, cols_used):\n",
    "    #import Financial Data\n",
    "    stock_dataset_df = load_data.import_dataset(stock_ticker, params.start_date, params.end_date)\n",
    "\n",
    "    # plot price comparison stock vs index\n",
    "    plot_data.plot_price_comparison_stocks(params.index_ticker, stock_ticker, stock_dataset_df, params.start_date, params.end_date)\n",
    "\n",
    "    # Generate images\n",
    "    feature_image_dataset_list, feature_price_dataset_list, feature_label_dataset_list, cols_used_count = image_transform.generate_features_lists(\n",
    "        stock_dataset_df, \n",
    "        cols_used,\n",
    "        params.transform_algo, \n",
    "        params.transformed_img_sz, \n",
    "        params.gaf_method, \n",
    "        params.sample_range)\n",
    "\n",
    "    images_array, labels_array = image_transform.create_images_array(feature_image_dataset_list, feature_label_dataset_list)\n",
    "\n",
    "    #Quick Sample Image Visualization\n",
    "    #Visualize Closing Price for one image in GAF or Markov:\n",
    "    # A darker patch indicates lower correlation between the different elements of the price time series, \n",
    "    # possibly due to higher volatility or noise. The opposite is true for the lighter patches.\n",
    "    if params.scenario == 0: plot_data.quick_view_images(images_array, cols_used_count, cols_used)\n",
    "\n",
    "    #Prepare and Load Data\n",
    "    images_array, labels_array = image_transform.squeeze_array(images_array, labels_array)\n",
    "\n",
    "    feature_image_dataset_list_f32, labels_scaled_list_f32 = image_transform.Generate_feature_image_to_f32(\n",
    "        labels_array, \n",
    "        images_array,\n",
    "        params.transformed_img_sz, \n",
    "        params.scaler)\n",
    "\n",
    "    train_loader, test_loader = load_data.Generate_Loaders(feature_image_dataset_list_f32,\n",
    "                                                labels_scaled_list_f32, test_size,\n",
    "                                                params.batch_size,\n",
    "                                                train_shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, stock_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process(train_loader, params):\n",
    "    #init cnn\n",
    "    net = neural_network.instantiate_net(params)\n",
    "\n",
    "    # train cnn\n",
    "    net = neural_network.Train(params, train_loader, net)\n",
    "\n",
    "    helper_functions.Save_Model(params.scenario, net)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Network Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process(net, test_loader, params, stock_ticker):\n",
    "    \n",
    "    # test\n",
    "    stack_input, predicted_list, actual_list, accuracy, stack_actual, stack_predicted  = neural_network.Test(test_loader,net)\n",
    "\n",
    "    # Plot image mean input values\n",
    "    plot_data.scatter_diagram_onevar_plot_mean(stack_input, stock_ticker)\n",
    "    \n",
    "    #compute stats\n",
    "    compute_stats.compute_and_report_error_stats(stack_actual, stack_predicted, stock_ticker)\n",
    "\n",
    "    #write to file\n",
    "    helper_functions.write_scenario_to_log_file(accuracy)\n",
    "\n",
    "    return stack_input, stack_actual, stack_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report stats results\n",
    "def report_external_test_stats(params, stock_dataset_df, \n",
    "                               test_stack_input, train_stack_input,\n",
    "                               test_stack_actual, test_stack_predicted):\n",
    "    \n",
    "    #test_stack_actual is the actual observation\n",
    "    #test_stack_predicted is the predicted observation\n",
    "\n",
    "    #print(\"test_stack_input.shape\",test_stack_input.shape,\"test_stack input\",test_stack_input)\n",
    "    #compute correl prices\n",
    "    compute_stats.stock_correlation_matrix(params.external_test_stock_ticker, stock_dataset_df)\n",
    "    #compute correl images\n",
    "    #print(\"trained input image shape\",train_stack_input.shape,\"test input image shape\",test_stack_input.shape)\n",
    "    image_series_correlations, image_series_mean_correlation = compute_stats.cross_stock_image_array_correlation2(test_stack_input,train_stack_input)\n",
    "\n",
    "    #compute cross correl\n",
    "    benchmark_stock_df = load_data.import_dataset(params.train_stock_ticker, params.start_date, params.end_date)\n",
    "    compute_stats.cross_stock_df_correlation(params.external_test_stock_ticker, params.train_stock_ticker,stock_dataset_df, benchmark_stock_df)\n",
    "\n",
    "    #compute stats\n",
    "    error_stats = compute_stats.compute_error_stats(test_stack_actual, test_stack_predicted)\n",
    "    print(f\"Inference Model Stats for {params.external_test_stock_ticker}\")\n",
    "    for key, value in error_stats.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "    return image_series_correlations, image_series_mean_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Brute Force Train and Test</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'parameters' has no attribute 'train_stock_ticker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#       Train and Test          #\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#generate training images\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m train_loader, test_loader, stock_dataset_df \u001b[38;5;241m=\u001b[39m generate_dataset_to_images_process(\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_stock_ticker\u001b[49m, \n\u001b[0;32m      9\u001b[0m                                                             params, \n\u001b[0;32m     10\u001b[0m                                                             params\u001b[38;5;241m.\u001b[39mtraining_test_size, \n\u001b[0;32m     11\u001b[0m                                                             params\u001b[38;5;241m.\u001b[39mtraining_cols_used)\n\u001b[0;32m     13\u001b[0m net \u001b[38;5;241m=\u001b[39m train_process(train_loader, params)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#test\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# set model to eval\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'parameters' has no attribute 'train_stock_ticker'"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "#       Train and Test          #\n",
    "#################################\n",
    "#instantiate params\n",
    "#params = parameters.Parameters()\n",
    "\n",
    "#generate training images\n",
    "train_loader, test_loader, stock_dataset_df = generate_dataset_to_images_process(params.train_stock_ticker, \n",
    "                                                            params, \n",
    "                                                            params.training_test_size, \n",
    "                                                            params.training_cols_used)\n",
    "\n",
    "net = train_process(train_loader, params)\n",
    "\n",
    "#test\n",
    "# set model to eval\n",
    "net  = neural_network.set_model_for_eval(net)\n",
    "\n",
    "test_stack_input, test_stack_actual, test_stack_predicted = test_process(net, test_loader, \n",
    "                                                                        params, \n",
    "                                                                        params.train_stock_ticker)\n",
    "\n",
    "#################################\n",
    "#       External Test           #\n",
    "#################################\n",
    "print (\"\\n\\nRun External Stock Tests:\")\n",
    "#load model\n",
    "PATH = f'./model_scen_{0}_full.pth'\n",
    "net = helper_functions.Load_Full_Model(PATH)\n",
    "\n",
    "#external test image generation\n",
    "cols_used = [\"Open\", \"High\"]\n",
    "train_loader, test_loader, stock_dataset_df = generate_dataset_to_images_process(params.external_test_stock_ticker, \n",
    "                                                            params, \n",
    "                                                            params.external_test_size, \n",
    "                                                            params.external_test_cols_used)\n",
    "\n",
    "#test\n",
    "external_test_stack_input, external_test_stack_actual, external_test_stack_predicted = test_process(net, \n",
    "                                                                                                    test_loader, \n",
    "                                                                                                    params,\n",
    "                                                                                                    params.external_test_stock_ticker)\n",
    "\n",
    "#report stats\n",
    "image_series_correlations, image_series_mean_correlation = report_external_test_stats(\n",
    "                                                    params, stock_dataset_df, \n",
    "                                                    test_stack_input, external_test_stack_input,\n",
    "                                                    external_test_stack_actual, external_test_stack_predicted)\n",
    "\n",
    "plot_data.plot_external_test_graphs(params, test_stack_input, external_test_stack_input,\n",
    "                            image_series_correlations, image_series_mean_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization for CNN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Optimization(output_conv_1, \n",
    "                     output_conv_2,\n",
    "                     dropout_probab,\n",
    "                     learning_rate,\n",
    "                     momentum,\n",
    "                     train_loader,test_loader,\n",
    "                     iteration=None):\n",
    "    \n",
    "    output_conv_1 = int(output_conv_1)\n",
    "    output_conv_2 = int(output_conv_2)\n",
    "    \n",
    "    net = neural_network.Net(output_conv_1=output_conv_1, output_conv_2=output_conv_2,dropout_probab=dropout_probab)\n",
    "    net.to(device)\n",
    "\n",
    "    neural_network.Train(learning_rate=learning_rate, train_loader=train_loader, net=net, momentum=momentum)\n",
    "\n",
    "    if iteration is not None:\n",
    "        helper_functions.Save_BayesOpt_Model(iteration,net)\n",
    "\n",
    "    stack_input, predicted, actual, accuracy, percentage_diffs, mean_percentage_diff = neural_network.Test(test_loader=test_loader,net=net)\n",
    "    \n",
    "    print(\"accuracy received\",accuracy)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Optimize():\n",
    "    \n",
    "    train_loader,test_loader = load_data.Generate_Train_And_Test_Loaders(feature_image_dataset_list_f32,labels_scaled_list_f32, test_size, batch_size=batch_size, train_shuffle=False)\n",
    "    \n",
    "    iteration_counter = [0]\n",
    "\n",
    "    def cnn_correct_pct_wrapper(output_conv_1, output_conv_2, learning_rate, dropout_probab, momentum):\n",
    "        result = CNN_Optimization(output_conv_1, output_conv_2, learning_rate, dropout_probab, momentum, train_loader, test_loader, iteration=iteration_counter[0])\n",
    "        iteration_counter[0] += 1\n",
    "        return result\n",
    "    #cnn_correct_pct = partial(CNN_Optimization, train_loader = train_loader, test_loader = test_loader)\n",
    "\n",
    "    # Bounded region of parameter space\n",
    "    pbounds = {'output_conv_1': (40, 80),\n",
    "               'output_conv_2': (8, 16), \n",
    "               'learning_rate': (0.00001, 0.0001), \n",
    "               'dropout_probab': (0.0, 0.5), \n",
    "               'momentum': (0.8, 1.0)}\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f = cnn_correct_pct_wrapper,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1,\n",
    "        )\n",
    "\n",
    "    #n_iter:steps of bayesian optimization you want to perform\n",
    "    #init_points:steps of random exploration\n",
    "    optimizer.maximize(init_points=10, n_iter=10,)\n",
    "\n",
    "    with open(f'optimizer_results.txt', 'a') as file:\n",
    "        for i, res in enumerate(optimizer.res):\n",
    "            print(f\"Iteration {i}: \\n\\t{res}\")\n",
    "            file.write(\"\\n\\nIteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "    optimizer_max_result = optimizer.max\n",
    "    print(\"optimizer result:\\n\",optimizer.max)\n",
    "\n",
    "    with open(f'optimizer_results.txt', 'a') as file:\n",
    "        file.write('\\noptimizer results:\\n' + str(optimizer_max_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out to not run bayesian opt\n",
    "#Optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to reconstruct Net model design. Enter optimizer_results.txt scenario variable results used to validate the net design model inference result matches the optimizer_results.txt output\n",
    "# dropout_probab = 0.4473033317519236\n",
    "# learning_rate = 1.7653979023280013e-05\n",
    "# momentum = 0.8078109566465765\n",
    "# output_conv_1 = int(46.7)\n",
    "# output_conv_2 = int(15.0)\n",
    "\n",
    "# net = Net(name='Classification Net', filter_size_1=filter_size_1, filter_size_2=filter_size_2,\n",
    "#             filter_size_3=filter_size_3, stride=stride,\n",
    "#             image_resolution_x=32,image_resolution_y=32,\n",
    "#             output_conv_1=output_conv_1, output_conv_2=output_conv_2,\n",
    "#             output_FC_1=output_FC_1, output_FC_2=output_FC_2,\n",
    "#             final_FCLayer_outputs=final_FCLayer_output,\n",
    "#             dropout_probab=dropout_probab)\n",
    "\n",
    "# net.to(device)\n",
    "# net.parameters()\n",
    "\n",
    "# net = Load_BayesOpt_Model(\"5\",net)\n",
    "\n",
    "# predicted_np, actual_np, accuracy, percentage_diffs, mean_percentage_diff, average_percentage_diff, df = Test_And_Report(net)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
